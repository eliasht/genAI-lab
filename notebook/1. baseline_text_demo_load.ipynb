{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../app/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_azure_openai_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding model\n",
    "\n",
    "init_azure_openai_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "# Get metadata for leaflets\n",
    "def get_leaflets_metadata(file_path):\n",
    "    # Extract brand name from file path\n",
    "    try:\n",
    "        brand = file_path.split(\"\\\\\")[-2]\n",
    "        model_name = file_path.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    except IndexError:\n",
    "        print(f\"file path : {file_path}\")\n",
    "    return {\"brand\": brand, \"model_name\": model_name}\n",
    "\n",
    "\n",
    "# Read all leaflets\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"leaflets/\", recursive=True, file_metadata=get_leaflets_metadata\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split documents into nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text splitters (https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/?h=sentencespli#sentencesplitter\n",
    "# Try automergingn (https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/)\n",
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "chunked_index = VectorStoreIndex(nodes=nodes, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "chunked_index.set_index_id(\"vector_chunked_index\")\n",
    "chunked_index.storage_context.persist(\"./storage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesmate-be",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
